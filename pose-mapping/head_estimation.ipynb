{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head Estimation, Pose Conversion & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Process Keypoints JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0, 1, 2, 3, 4, 5, 6, 11, 12]\n",
    "\n",
    "def access_elements(data, indices):\n",
    "    return [data[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load person_keypoints_train2017.json and extract poses which have all of their body keypoints\n",
    "'''\n",
    "\"keypoints\": [\n",
    "                        \"nose\",\n",
    "                        \"left_eye\",\n",
    "                        \"right_eye\",\n",
    "                        \"left_ear\",\n",
    "                        \"right_ear\",\n",
    "                        \n",
    "                        \"left_shoulder\",\n",
    "                        \"right_shoulder\",\n",
    "                        \"left_elbow\",\n",
    "                        \"right_elbow\",\n",
    "                        \"left_wrist\",\n",
    "                        \"right_wrist\",\n",
    "\n",
    "                        \"left_hip\",\n",
    "                        \"right_hip\",\n",
    "                        \"left_knee\",\n",
    "                        \"right_knee\",\n",
    "                        \"left_ankle\",\n",
    "                        \"right_ankle\"\n",
    "                    ],\n",
    "'''\n",
    "\n",
    "def extract_poses(file_path, scale=2):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    poses_list = []\n",
    "\n",
    "    for annotation in data['annotations']:\n",
    "        visibility = access_elements(annotation['keypoints'][2::3], indices)\n",
    "\n",
    "        if 0 not in visibility:\n",
    "            keypoints_x = access_elements(annotation['keypoints'][0::3], indices)\n",
    "            keypoints_y = access_elements(annotation['keypoints'][1::3], indices)\n",
    "\n",
    "            max_x = max(keypoints_x)\n",
    "            min_x = min(keypoints_x)\n",
    "            max_y = max(keypoints_y)\n",
    "            min_y = min(keypoints_y)\n",
    "\n",
    "            height = max_y - min_y\n",
    "            width = max_x - min_x\n",
    "\n",
    "            if height > width:\n",
    "                keypoints_x = [((x - min_x) / height - 0.5) * scale for x in keypoints_x]\n",
    "                keypoints_y = [((y - min_y) / height - 0.5) * scale for y in keypoints_y]\n",
    "            else:\n",
    "                keypoints_x = [((x - min_x) / width - 0.5) * scale for x in keypoints_x]\n",
    "                keypoints_y = [((y - min_y) / width - 0.5) * scale for y in keypoints_y]\n",
    "\n",
    "            poses_list.append((keypoints_x[5:] + keypoints_y[5:], keypoints_x[:5] + keypoints_y[:5]))\n",
    "\n",
    "    return poses_list\n",
    "\n",
    "# File path to the JSON file\n",
    "file_path = 'person_keypoints_train2017.json'\n",
    "\n",
    "# Extract keypoints\n",
    "extracted_poses = extract_poses(file_path)\n",
    "\n",
    "# Print the result\n",
    "result_len = len(extracted_poses)\n",
    "print(extracted_poses[:5])\n",
    "print(f\"Number of extracted poses: {result_len}\")\n",
    "\n",
    "split = 0.8\n",
    "\n",
    "# Save the result\n",
    "with open(\"train.json\", \"w\") as train:\n",
    "    json.dump(extracted_poses[:int(result_len * split)], train)\n",
    "\n",
    "with open(\"test.json\", \"w\") as test:\n",
    "    json.dump(extracted_poses[int(result_len * split):], test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "print(f\"MPS Support: {torch.backends.mps.is_built()}\")\n",
    "print(f\"MPS Availability: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataloader and Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        train_path = data_dir + \"/train.json\"\n",
    "\n",
    "        with open(train_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            data_len = len(data)\n",
    "            self.data = data[:int(data_len * split)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index][0]), torch.tensor(self.data[index][1])\n",
    "\n",
    "\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        train_path = data_dir + \"/train.json\"\n",
    "\n",
    "        with open(train_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            data_len = len(data)\n",
    "            self.data = data[int(data_len * split):]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index][0]), torch.tensor(self.data[index][1])\n",
    "    \n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        test_path = data_dir + \"/test.json\"\n",
    "\n",
    "        with open(test_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index][0]), torch.tensor(self.data[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_dataset = TrainDataset(data_dir='.')\n",
    "val_dataset = ValDataset(data_dir='.')\n",
    "test_dataset = TestDataset(data_dir='.')\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class HeadEstimator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HeadEstimator, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 64)\n",
    "        self.fc2 = nn.Linear(64, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        out = self.fc4(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Val Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainloader, model, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (inputs, targets) in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        #criterion = criterion.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.cpu().item()\n",
    "\n",
    "    return total_loss / (batch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(valloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in tqdm(enumerate(valloader), total = len(valloader)):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            #criterion = criterion.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, targets)\n",
    "            \n",
    "            total_loss += loss.cpu().item()\n",
    "\n",
    "    return total_loss/(batch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 5e-4\n",
    "epochs = 50\n",
    "\n",
    "trainLoader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "valLoader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "model = HeadEstimator()\n",
    "model = model.to(device)\n",
    "\n",
    "def loss(preds, targets):\n",
    "    mse = torch.mean((preds - targets) ** 2)\n",
    "\n",
    "    eye_dist = torch.mean((preds[:, 1:3] - targets[:, 1:3]) ** 2 \n",
    "                            + (preds[:, 6:8] - targets[:, 6:8]) ** 2)\n",
    "    \n",
    "    ear_dist = torch.mean((preds[:, 3:5] - targets[:, 3:5]) ** 2 \n",
    "                            + (preds[:, 8:10] - targets[:, 8:10]) ** 2)\n",
    "\n",
    "    return mse + eye_dist + ear_dist\n",
    "\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "\n",
    "history = {'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(trainLoader, model, criterion, optimizer, scheduler, device)\n",
    "    val_loss = val_model(valLoader, model, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if (epoch+1) % 5 == 0 or epoch == epochs-1:\n",
    "        print(\"Saving checkpoint...\")\n",
    "        torch.save(model.state_dict(), f'./model.pth')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), history['train_loss'], label='Train Loss', color='red')\n",
    "plt.plot(range(epochs), history['val_loss'], label='Val Loss', color='blue')\n",
    "\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(testloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in tqdm(enumerate(testloader), total = len(testloader)):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, targets)\n",
    "            \n",
    "            total_loss += loss.cpu().item()\n",
    "\n",
    "    return total_loss/(batch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "print(\"Testing...\")\n",
    "test_loss = test_model(testLoader, model, criterion, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference and Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, device, x):\n",
    "    model.eval()\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        \n",
    "    return pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pose(pose, scale=2):\n",
    "    \n",
    "    keypoints_x = [point[0] for point in pose]\n",
    "    keypoints_y = [point[1] for point in pose]\n",
    "\n",
    "    min_x = min(keypoints_x)\n",
    "    max_x = max(keypoints_x)\n",
    "    min_y = min(keypoints_y)\n",
    "    max_y = max(keypoints_y)\n",
    "    \n",
    "    height = max_y - min_y\n",
    "    width = max_x - min_x\n",
    "\n",
    "    if height > width:\n",
    "        keypoints_x = [((x - min_x) / height - 0.5) * scale for x in keypoints_x]\n",
    "        keypoints_y = [((y - min_y) / height - 0.5) * scale for y in keypoints_y]\n",
    "        original_scale = height\n",
    "    else:\n",
    "        keypoints_x = [((x - min_x) / width - 0.5) * scale for x in keypoints_x]\n",
    "        keypoints_y = [((y - min_y) / width - 0.5) * scale for y in keypoints_y]\n",
    "        original_scale = width\n",
    "\n",
    "    \n",
    "    print('keypoints_x = \"', keypoints_x, '\"')\n",
    "    print('keypoints_y = \"', keypoints_y, '\"\\n')\n",
    "\n",
    "    pose = [(x, y) for x, y in zip(keypoints_x, keypoints_y)]\n",
    "\n",
    "    return pose, min_x, min_y, original_scale, original_scale\n",
    "\n",
    "def denormalize_pose(pose, min_x, min_y, width, height, scale=2):\n",
    "    pose = [((x / scale + 0.5) * width + min_x, (y / scale + 0.5) * height + min_y) for (x, y) in pose]\n",
    "\n",
    "    return pose\n",
    "\n",
    "def resize_pose(pose, resize_width=200, resize_height=200, padding=10, scale=2):\n",
    "    pose, _, _, _, _ = normalize_pose(pose, scale)\n",
    "    pose = [((x / scale + 0.5) * resize_width + padding, (y / scale + 0.5) * resize_height + padding) for (x, y) in pose]\n",
    "\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "coco (original):\n",
    "0 : nose\n",
    "1 2 : eyes\n",
    "3 4 : ears\n",
    "5 7 9 : left arm\n",
    "6 8 10: right arm\n",
    "11 13 15: left leg\n",
    "12 14 16: right leg\n",
    "\n",
    "mpii (ours):\n",
    "13 14 15 : left arm\n",
    "12 11 10 : right arm\n",
    "3 4 5 : left leg\n",
    "2 1 0 : right leg\n",
    "7 : neck\n",
    "9 8 : head\n",
    "\n",
    "coco (ours):\n",
    "0 : nose\n",
    "1 : neck\n",
    "2 3 4 : right arm\n",
    "5 6 7 : left arm\n",
    "8 9 10 : right leg\n",
    "11 12 13 : left leg\n",
    "14 15 : eyes\n",
    "16 17 : ears\n",
    "'''\n",
    "\n",
    "def convert_pose(model, device, pose):\n",
    "    # re-order keypoints to match neural network input\n",
    "    reordered_pose = [pose[13], pose[12], pose[14], pose[11], pose[15], pose[10], pose[3], pose[2], pose[4], pose[1], pose[5], pose[0]]\n",
    "\n",
    "    upper_body = access_elements(reordered_pose, [0, 1, 6, 7]) + [((pose[8][0] + pose[9][0]*2)/3, (pose[8][1] + pose[9][1]*2)/3)]\n",
    "    upper_body, min_x, min_y, width, height = normalize_pose(upper_body)\n",
    "\n",
    "    input_x = [point[0] for point in upper_body[:4]]\n",
    "    input_y = [point[1] for point in upper_body[:4]]\n",
    "\n",
    "    input_pose = torch.tensor([input_x + input_y])\n",
    "\n",
    "    # head inference\n",
    "    head = inference(model, device, input_pose)[0].cpu().tolist()\n",
    "    head_points = [(x, y) for x, y in zip(head[:5], head[5:])]\n",
    "\n",
    "    # denormalize head keypoints\n",
    "    head_points = denormalize_pose(head_points, min_x, min_y, width, height)\n",
    "\n",
    "    # insert head keypoints to the pose\n",
    "    reordered_pose = [head_points[0], pose[7], pose[12], pose[11], pose[10], pose[13], pose[14], pose[15], pose[2], pose[1], pose[0], pose[3], pose[4], pose[5],\n",
    "                      head_points[1], head_points[2], head_points[3], head_points[4]]\n",
    "\n",
    "    return reordered_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mpii_link_pairs = [[0, 1], [1, 2], [2, 6], \n",
    "              [3, 6], [3, 4], [4, 5], \n",
    "              [6, 7], [7,12], [11, 12], \n",
    "              [10, 11], [7, 13], [13, 14],\n",
    "              [14, 15],[7, 8],[8, 9]]\n",
    "\n",
    "mpii_link_color = [(0, 0, 255), (0, 0, 255), (0, 0, 255),\n",
    "              (0, 255, 0), (0, 255, 0), (0, 255, 0),\n",
    "              (0, 255, 255), (0, 0, 255), (0, 0, 255),\n",
    "              (0, 0, 255), (0, 255, 0), (0, 255, 0),\n",
    "              (0, 255, 0), (0, 255, 255), (0, 255, 255)]\n",
    "\n",
    "mpii_point_color = [(255,0,0),(0,255,0),(0,0,255), \n",
    "               (128,0,0), (0,128,0), (0,0,128),\n",
    "               (255, 255, 0),(0,255,255),(255, 0, 255),\n",
    "               (128,128,0),(0, 128, 128),(128,0,128),\n",
    "               (128,255,0),(128,128,128),(255,128,0),\n",
    "               (255,0,128),(255,255,255)]\n",
    "\n",
    "\n",
    "coco_link_pairs = [[0, 1], [1, 2], [2, 3], \n",
    "              [3, 4], [1, 5], [5, 6], \n",
    "              [6, 7], [1, 8], [8, 9], \n",
    "              [9, 10], [1, 11], [11, 12],\n",
    "              [12, 13],[0, 14],[14, 16], [0, 15], [15, 17]]\n",
    "\n",
    "coco_link_color = [(0, 0, 255), (0, 0, 255), (0, 0, 255),\n",
    "              (0, 255, 0), (0, 255, 0), (0, 255, 0),\n",
    "              (0, 255, 255), (0, 0, 255), (0, 0, 255),\n",
    "              (0, 0, 255), (0, 255, 0), (0, 255, 0),\n",
    "              (0, 255, 0), (0, 255, 255), (0, 255, 255), (128, 128, 0), (128, 0, 128)]\n",
    "\n",
    "coco_point_color = [(255,0,0),(0,255,0),(0,0,255), \n",
    "               (128,0,0), (0,128,0), (0,0,128),\n",
    "               (255, 255, 0),(0,255,255),(255, 0, 255),\n",
    "               (128,128,0),(0, 128, 128),(128,0,128),\n",
    "               (128,255,0),(128,128,128),(255,128,0),\n",
    "               (255,0,128),(255,255,255), (128, 128, 0), (128, 0, 128)]\n",
    "\n",
    "'''\n",
    "mpii (ours):\n",
    "2 1 0 : right leg\n",
    "3 4 5 : left leg\n",
    "12 11 10 : right arm\n",
    "13 14 15 : left arm\n",
    "7 : neck\n",
    "9 8 : head\n",
    "\n",
    "coco (ours):\n",
    "0 : nose\n",
    "1 : neck\n",
    "2 3 4 : right arm\n",
    "5 6 7 : left arm\n",
    "8 9 10 : right leg\n",
    "11 12 13 : left leg\n",
    "14 15 : eyes\n",
    "16 17 : ears\n",
    "'''\n",
    "\n",
    "def vis_pose(image_path, pose, link_pairs, link_color, point_color):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    pose = [(int(x), int(y)) for (x, y) in pose]\n",
    "\n",
    "    for idx, pair in enumerate(link_pairs):\n",
    "        if pose[pair[0]] != (0, 0) and pose[pair[1]] != (0, 0):\n",
    "            cv2.line(image, pose[pair[0]], pose[pair[1]], link_color[idx], 2)\n",
    "\n",
    "    for idx, point in enumerate(pose):\n",
    "        if point != (0, 0):\n",
    "            cv2.putText(image, str(idx), point, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            cv2.circle(image, point, 5, point_color[idx], thickness=-1)\n",
    "\n",
    "    cv2.imshow(\"image\", image)\n",
    "    cv2.moveWindow(\"image\", 0, 0)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    '''\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "\n",
    "def vis_pose_mpii(image_path, pose):\n",
    "    vis_pose(image_path, pose, mpii_link_pairs, mpii_link_color, mpii_point_color)\n",
    "\n",
    "\n",
    "def vis_pose_coco(image_path, pose):\n",
    "    vis_pose(image_path, pose, coco_link_pairs, coco_link_color, coco_point_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def vis_data(model, device, data, image_dir_path='./affordance_data/data'):\n",
    "    data = data.split(' ')\n",
    "    image_path = os.path.join(image_dir_path, data[0])\n",
    "    pose_data = data[1:-1]\n",
    "    pose_data = [eval(x) for x in pose_data]\n",
    "\n",
    "    pose = []\n",
    "    for i in range(0, len(pose_data), 2):\n",
    "        pose.append((pose_data[i], pose_data[i+1]))\n",
    "\n",
    "    vis_pose_mpii(image_path, pose)\n",
    "\n",
    "    pose = convert_pose(model, device, pose)\n",
    "    vis_pose_coco(image_path, pose)\n",
    "    vis_pose_coco(image_path, resize_pose(pose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pose Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint\n",
    "model = HeadEstimator()\n",
    "model.load_state_dict(torch.load(f'./model.pth'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './affordance_data/trainlist.txt'\n",
    "train_data = []\n",
    "with open(train_data_path, 'r') as f:\n",
    "    train_data = list(f.readlines())\n",
    "\n",
    "for data in train_data[1000:]:\n",
    "    vis_data(model, device, data)\n",
    "    # input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
