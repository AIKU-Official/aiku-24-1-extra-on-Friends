{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from config import Config\n",
    "from model import PoseClassifier, VariationalAutoencoder\n",
    "from train_vae import generate_pose\n",
    "from vis_pose import vis_pose\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = None\n",
    "copied_image = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InferenceModule(object):\n",
    "    def __init__(self, cfg, classifier, vae):\n",
    "        self.classifier = classifier\n",
    "        self.vae = vae\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.cluster_keypoints_list = []\n",
    "        with open(os.path.join('./affordance_data', 'centers_30.txt'), 'r') as f:\n",
    "            cluster_data_list = list(f.readlines())\n",
    "        for cluster_data in cluster_data_list:\n",
    "            cluster_data = cluster_data.split(' ')[:-1]\n",
    "            cluster_data = [float(x) for x in cluster_data]\n",
    "            cluster_keypoints = []\n",
    "            for i in range(0, len(cluster_data), 2):\n",
    "                cluster_keypoints.append((cluster_data[i], cluster_data[i+1]))\n",
    "            cluster_keypoints = cluster_keypoints[:-1]\n",
    "            self.cluster_keypoints_list.append(torch.tensor(cluster_keypoints))\n",
    "        self.cluster_keypoints_list = torch.stack(self.cluster_keypoints_list)\n",
    "    \n",
    "    def inference(self, image, target_point):\n",
    "\n",
    "        width, height = image.size\n",
    "        crop_box = ((target_point[0] - (height // 2)), (target_point[1] - (height // 2)), (target_point[0] + (height // 2)), (target_point[1] + (height // 2)))\n",
    "        zoom_box = ((target_point[0] - (height // 4)), (target_point[1] - (height // 4)), (target_point[0] + (height // 4)), (target_point[1] + (height // 4)))\n",
    "        image_crop = image.crop(crop_box)\n",
    "        image_zoom = image.crop(zoom_box)\n",
    "\n",
    "        image_tensor = self.transform(image).unsqueeze(0)\n",
    "        image_crop_tensor = self.transform(image_crop).unsqueeze(0)\n",
    "        image_zoom_tensor = self.transform(image_zoom).unsqueeze(0)\n",
    "\n",
    "        pose_label = self.classifier(image_tensor, image_crop_tensor, image_zoom_tensor)[0]\n",
    "        pose_index = torch.argmax(pose_label)\n",
    "        base_pose = self.cluster_keypoints_list[pose_index]\n",
    "        one_hot_pose = [0.0] * pose_label.shape[0]\n",
    "        one_hot_pose[pose_index] = 1.0\n",
    "        one_hot_pose = torch.tensor(one_hot_pose).unsqueeze(0)\n",
    "\n",
    "        latent_vector = torch.randn((1, self.cfg.latent_dim))\n",
    "\n",
    "        sclae_deformation = self.vae.decoder(latent_vector, one_hot_pose, image_tensor, image_crop_tensor, image_zoom_tensor)\n",
    "        scale = sclae_deformation[:, :2]\n",
    "        deformation = sclae_deformation[:, 2:]\n",
    "\n",
    "        pose = generate_pose(base_pose.unsqueeze(0), scale, deformation, torch.tensor(target_point).unsqueeze(0))\n",
    "        base_pose = generate_pose(base_pose.unsqueeze(0), scale, torch.zeros(deformation.size()), torch.tensor(target_point).unsqueeze(0))\n",
    "        return pose, base_pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "cfg = Config()\n",
    "classifier = PoseClassifier(cfg).to(device)\n",
    "vae = VariationalAutoencoder(cfg).to(device)\n",
    "\n",
    "\n",
    "classifier_checkpoint = \"./checkpoints/experiment11/model_38_12_[0.16124030947685242, 0.2966408133506775, 0.39974159002304077, 0.49095603823661804, 0.5671834349632263].pt\"\n",
    "vae_checkpoint = \"./checkpoints/experiment11/model_14_1871_1391.pt\"\n",
    "\n",
    "classifier.load_state_dict(torch.load(classifier_checkpoint, map_location='cpu'))\n",
    "vae.load_state_dict(torch.load(vae_checkpoint, map_location='cpu'))\n",
    "\n",
    "model = InferenceModule(cfg, classifier, vae)\n",
    "\n",
    "\n",
    "image_path = 'test/test.jpg'\n",
    "target_point = (950, 450)\n",
    "\n",
    "image = Image.open(image_path)\n",
    "pose, base_pose = model.inference(image, target_point)\n",
    "pose = pose[0].tolist()\n",
    "base_pose = base_pose[0].tolist()\n",
    "pose = [(round(x[0]), round(x[1])) for x in pose]\n",
    "base_pose = [(round(x[0]), round(x[1])) for x in base_pose]\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "copied_image = image.copy()\n",
    "cv2.circle(copied_image, target_point, 20, (0, 0, 255), -1)\n",
    "plt.imshow(copied_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "copied_image = image.copy()\n",
    "cv2.circle(copied_image, target_point, 20, (0, 0, 255), -1)\n",
    "\n",
    "base_pose_image = vis_pose(image_path, base_pose, show=False)\n",
    "base_pose_image = cv2.cvtColor(base_pose_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "pose_image = vis_pose(image_path, pose, show=False)\n",
    "pose_image = cv2.cvtColor(pose_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plot_image = cv2.vconcat([copied_image, base_pose_image, pose_image])\n",
    "plt.imshow(plot_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./affordance_data/testlist.txt', 'r') as f:\n",
    "    test_data_list = list(f.readlines())\n",
    "test_image_list = [x.split(' ')[0] for x in test_data_list]\n",
    "import random\n",
    "test_image_name = random.sample(test_image_list, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_dir_path = './affordance_data/data'\n",
    "image_path = os.path.join(test_image_dir_path, test_image_name)\n",
    "target_point = (800, 300)\n",
    "\n",
    "image = Image.open(image_path)\n",
    "pose, base_pose = model.inference(image, target_point)\n",
    "pose = pose[0].tolist()\n",
    "base_pose = base_pose[0].tolist()\n",
    "pose = [(round(x[0]), round(x[1])) for x in pose]\n",
    "base_pose = [(round(x[0]), round(x[1])) for x in base_pose]\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "copied_image = image.copy()\n",
    "cv2.circle(copied_image, target_point, 20, (0, 0, 255), -1)\n",
    "plt.imshow(copied_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "copied_image = image.copy()\n",
    "cv2.circle(copied_image, target_point, 20, (0, 0, 255), -1)\n",
    "\n",
    "base_pose_image = vis_pose(image_path, base_pose, show=False)\n",
    "base_pose_image = cv2.cvtColor(base_pose_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "pose_image = vis_pose(image_path, pose, show=False)\n",
    "pose_image = cv2.cvtColor(pose_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plot_image = cv2.vconcat([copied_image, base_pose_image, pose_image])\n",
    "plt.imshow(plot_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose-generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
