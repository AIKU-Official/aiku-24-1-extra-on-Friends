{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for demo\n",
    "- 필요한 파일: 소스(인물 사진), 장면 이미지, 포즈 센터 및 포즈 키포인트\n",
    "- keypoint result를 example_imgs/keypoints.txt 파일에 위치하도록 저장하기\n",
    "- pose center 값 \"position\" 변수에 저장  \n",
    "\n",
    "\n",
    "- 나머지 source/target 파일 경로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 배경 이미지 - 즉, 장면 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_path = './example_imgs/friends_5.png'\n",
    "position = (350, 550)\n",
    "\n",
    "try:\n",
    "    # Open the image file\n",
    "    img = Image.open(background_path)\n",
    "    w, h = img.size\n",
    "\n",
    "    # Check the image mode and convert if necessary\n",
    "    if img.mode == 'RGBA':\n",
    "        img = img.convert('RGB')\n",
    "    elif img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "\n",
    "    # Plot the point on the image\n",
    "    plt.scatter([position[0]], [position[1]], c='red', s=100)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save the corrected image if there was an issue\n",
    "    corrected_image_path = background_path\n",
    "    img.save(corrected_image_path)\n",
    "    print(f\"Corrected image saved at: {corrected_image_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Insert 할 인물 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = './example_imgs/harrystyles.png'\n",
    "\n",
    "img = Image.open(source_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키포인트파일 경로 및 결과 경로\n",
    "keypoints_path = './example_imgs/keypoints_6.txt'\n",
    "save_path = './example_imgs/CFLD_result_6.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd pose-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_demo_server import inference\n",
    "path = '.' + background_path\n",
    "# 'generated_pose' is final output!\n",
    "generated_pose, base_pose, pose_image, base_pose_image, position_marked_image = inference(path, position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pose_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(base_pose_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoint 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadEstimator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HeadEstimator, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 64)\n",
    "        self.fc2 = nn.Linear(64, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        out = self.fc4(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeadEstimator()\n",
    "state_dict = torch.load('./pose_generation/model.pth', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose-mapping.utils import convert_pose, resize_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = convert_pose(model, 'cpu', generated_pose)\n",
    "pose = resize_pose(pose) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del resize_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save keypoints as file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_x = [round(point[0]+10, 6) for point in pose]\n",
    "keypoints_y = [round(point[1]+10, 6) for point in pose]\n",
    "\n",
    "keypoints_y_str = f'{keypoints_y}'\n",
    "keypoints_x_str = f'{keypoints_x}'\n",
    "\n",
    "with open('../example_imgs/keypoints_6.txt', \"w\") as file:\n",
    "    file.write(keypoints_y_str + '\\n')\n",
    "    file.write(keypoints_x_str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수정된 키포인트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points(coordinates):\n",
    "    x_coords, y_coords = keypoints_x, keypoints_y\n",
    "\n",
    "    # Plotting the coordinates\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(x_coords, y_coords, c='blue', marker='o')\n",
    "    plt.title('Scatter Plot of Given Coordinates with y=x Line')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "\n",
    "    # Set the aspect ratio of the plot to be equal\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "    # Set the same scaling for both axes\n",
    "    min_val = min(0, 1000)\n",
    "    max_val = max(0, 1000)\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "\n",
    "    # Plot the y=x line\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_points(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CFLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../CFLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path_an = '.' + source_path\n",
    "keypoints_path_an = '.' + keypoints_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python clfd_app.py \\\n",
    "    --source_path $source_path_an \\\n",
    "        --keypoints_path $keypoints_path_an --save_path $'../example_imgs/CFLD_result_6.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../sber-swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = '../example_imgs/CFLD_result_7.png'\n",
    "save_path = '../example_imgs/SWAP_result_7.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Please check your CUDA setup.\")\n",
    "\n",
    "device_index = 4\n",
    "num_cuda_devices = torch.cuda.device_count()\n",
    "if device_index >= num_cuda_devices:\n",
    "    raise RuntimeError(f\"Invalid CUDA device index: {device_index}. Available devices: {num_cuda_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python swap_app.py --target_path {target_path} --source_path {'.' + source_path} --save_path {save_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import supervision as sv\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# weight 불러오기\n",
    "CHECKPOINT_PATH = 'SAM/weights/sam_vit_h_4b8939.pth'\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device='cpu')\n",
    "# mask_generator 생성\n",
    "mask_generator = SamAutomaticMaskGenerator(sam) # device 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from rembg import remove\n",
    "import numpy as np\n",
    "\n",
    "def overlay_images(image_path1, image2_path):\n",
    "    base_image = Image.open(image_path1).convert(\"RGBA\")\n",
    "    image2 = Image.open(image2_path).convert('RGB')\n",
    "    overlay_width, overlay_height = image2.size\n",
    "\n",
    "    # image2 = np.array(image2)\n",
    "\n",
    "    # remove background using segment-anythig\n",
    "\n",
    "    image_bgr = cv2.imread(image2_path)\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    sam_result = mask_generator.generate(image_rgb)\n",
    "\n",
    "    object_mask = sorted(sam_result, key=lambda x: x['area'], reverse=True)[1]['segmentation']\n",
    "    # extracted_image = image2 * np.stack([object_mask]*3, axis=-1)\n",
    "    return object_mask\n",
    "\n",
    "def overlay_images_2(image_path1, image2_path, object_mask, position):\n",
    "    # overlay_image = remove(img2) # 배경 제거\n",
    "    base_image = Image.open(image_path1).convert(\"RGBA\")\n",
    "    image2 = Image.open(image2_path).convert('RGB')\n",
    "    overlay_width, overlay_height = image2.size\n",
    "\n",
    "    # display image\n",
    "    overlay_center = (position[0] - overlay_width // 2, position[1] - overlay_height // 2)\n",
    "    # extracted_image = Image.fromarray(extracted_image)\n",
    "\n",
    "    mask_image = Image.fromarray(object_mask).convert('L')\n",
    "    base_image.paste(image2, overlay_center, mask=mask_image)\n",
    "\n",
    "    return base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG = overlay_images(background_path, save_path[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = (1000, 650) # 위치 조정이 필요하다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT = overlay_images_2(background_path, save_path[1:], IMG, position)\n",
    "RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CFLD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
